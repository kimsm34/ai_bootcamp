{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aaf06b",
   "metadata": {
    "id": "74aaf06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from vision_model_final_another.ipynb\n"
     ]
    }
   ],
   "source": [
    "from face_detector.face_detector import DnnDetector\n",
    "import sys\n",
    "\n",
    "import import_ipynb\n",
    "# 원하는 모델로 바꾸어 사용하기\n",
    "from vision_model_final_another import my_model\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c3f295",
   "metadata": {
    "id": "21c3f295"
   },
   "outputs": [],
   "source": [
    "# (주의) OpenCV는 Colab에서 제대로 실행되지 않음.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6dbcc1",
   "metadata": {
    "id": "6b6dbcc1"
   },
   "outputs": [],
   "source": [
    "# 사용할 모델 선언하기\n",
    "my_model = my_model().to(device)\n",
    "\n",
    "# train이 아닌, evaluation 과정\n",
    "my_model.eval()\n",
    "\n",
    "# 기존에 학습한 모델 불러오기. XXXXXXXXXXXX에 epoch 번호 작성.\n",
    "#path = 'checkpoint/model_weights/weights_epoch_' + '33' + '.pth.tar'\n",
    "\n",
    "path = 'epoch_67.pt'\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "my_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Face detection을 위한 CascadeClassifier 모델 불러오기\n",
    "#path = \"haarcascade_frontalface_default.xml\"\n",
    "#face_detector = cv2.CascadeClassifier(path)\n",
    "\n",
    "sys.path.insert(1, 'face_detector')\n",
    "face_detector = DnnDetector('face_detector')\n",
    "blue = (255, 0, 0)\n",
    "green = (0, 255, 0)\n",
    "red = (0, 0, 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456ac218",
   "metadata": {
    "id": "456ac218"
   },
   "outputs": [],
   "source": [
    "def sort_vec(vec):\n",
    "    \"\"\"\n",
    "    7차원 감정 벡터의 순서를 일치시키기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    vec : numpy.ndarray, shape-(7,)\n",
    "        vision 모델의 출력값인 7차원 벡터.\n",
    "        fer2013 dataset의 emotion label의 순서대로 구한 감정 벡터. \n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    numpy.ndarray, shape-(7,)\n",
    "        NLP dataset의 emotion index의 순서대로 재정렬한 감정 벡터\n",
    "    \n",
    "    참고사항\n",
    "    -------\n",
    "    (fer2013 dataset)\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust', \n",
    "        2: 'Fear', \n",
    "        3: 'Happy', \n",
    "        4: 'Sad', \n",
    "        5: 'Surprise', \n",
    "        6: 'Neutral'\n",
    "        \n",
    "    (NLP dataset)\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'  \n",
    "    \"\"\"\n",
    "    ## 여기에 코드 작성\n",
    "    v = [vec[2], vec[5], vec[0], vec[4], vec[6], vec[3], vec[1]]\n",
    "    '''\n",
    "    sort = {\n",
    "        0: vec[2],\n",
    "        1: vec[5],\n",
    "        2: vec[0],\n",
    "        3: vec[4],\n",
    "        4: vec[6],\n",
    "        5: vec[3],\n",
    "        6: vec[1]\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    return np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6345bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 0 4 6 3 1]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([0,1,2,3,4,5,6])\n",
    "print(sort_vec(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac5936b",
   "metadata": {
    "id": "8ac5936b"
   },
   "outputs": [],
   "source": [
    "def get_label_emotion(label):\n",
    "    \"\"\"\n",
    "    label 값에 대응되는 감정의 이름 문자열을 구하기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    label : int\n",
    "        emotion label 번호\n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    String\n",
    "        label 번호에 대응되는 감정의 이름 문자열\n",
    "    \n",
    "    참고사항\n",
    "    -------\n",
    "    (NLP dataset)\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'      \n",
    "    \"\"\"\n",
    "    ## 여기에 코드 작성\n",
    "    emotion_labels = { \n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust' }\n",
    "    return emotion_labels[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a18d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry\n"
     ]
    }
   ],
   "source": [
    "x = np.int64(2)\n",
    "print(get_label_emotion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b87a6c",
   "metadata": {
    "id": "88b87a6c"
   },
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(A, B):\n",
    "    \"\"\"\n",
    "    두 벡터 A, B의 코사인 유사도를 구하기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    A : numpy.ndarray, shape-(N,)\n",
    "    B : numpy.ndarray, shape-(N,)\n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    numpy.float64\n",
    "        두 벡터 A, B의 코사인 유사도 값      \n",
    "    \"\"\"\n",
    "    ## 여기에 코드 작성\n",
    "    return dot(A, B) / (norm(A) * norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7a134c",
   "metadata": {
    "id": "ae7a134c"
   },
   "outputs": [],
   "source": [
    "def predict_video(nlp_vec, sentence):\n",
    "    \"\"\"\n",
    "    웹캠을 통해 받아온 실시간 영상 속에서\n",
    "    1) CascadeClassifier (혹은 다른 모델)을 통해 얼굴을 탐지하고\n",
    "    2) Mini_Xception (혹은 다른 모델)을 통해 얼굴 표정으로부터 7차원 감정 벡터를 추출하여\n",
    "    3) sort_vec 함수를 통해 2)에서 구한 감정 벡터의 순서를 재정렬한 후\n",
    "    4) cos_sim 함수를 이용하여 입력받은 문장에서 추출한 7차원 감정 벡터와의 코사인 유사도를 계산.\n",
    "    5) OpenCV 라이브러리를 사용하여, 문장과 표정에서 추출한 각 감정, 그리고 표정 연기에 대한 점수(코사인 유사도)를 window 상에 시각화.\n",
    "    \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    nlp_vec : 입력한 문장에서 추출한 7차원 감정 벡터\n",
    "    sentence : 사용자가 표정 연기 연습의 목적으로 입력한 문장\n",
    "    \"\"\"\n",
    "    font_kor = ImageFont.truetype(\"malgun.ttf\", 30)\n",
    "    # OpenCV 실시간 웹캠 영상 불러오기\n",
    "    # 동영상을 저장(write)할 파일 경로\n",
    "    write_path = \"me_webcam.mp4\"\n",
    "\n",
    "    # VideoCapture, 기본 노트북 웹캠의 index는 0.\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # 프레임의 사이즈 계산 (height, width 구하기)\n",
    "    ## 여기에 코드 작성\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "    size = (width, height)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc('M','P','4','V')\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # 문장의 최대 확률 감정\n",
    "            ##\n",
    "            #nlp_vec = sort_vec(nlp_vec)\n",
    "            ##\n",
    "           \n",
    "            emotion_max = np.argmax(nlp_vec)\n",
    "            #print(type(emotion_max))\n",
    "            \n",
    "            nlp_percentage = np.round(nlp_vec[emotion_max], 2)\n",
    "            nlp_emotion_label = get_label_emotion(emotion_max)\n",
    "            \n",
    "            # 한글 문장 출력을 위한 하단 색 띠 만들기\n",
    "            frame_pil = frame\n",
    "            frame_pil[height-70 : height, 0 : width] = (50, 100, 50)\n",
    "            \n",
    "            # 한글 문장 출력을 위한 PIL 라이브러리 사용\n",
    "            ## 여기에 코드 작성\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            cv2.putText(frame_pil, \"emotion : \" + nlp_emotion_label + \"(\" + str(nlp_percentage) + \")\", (0, int(height-20)), font, 1, (255, 255, 255))\n",
    "            \n",
    "            \n",
    "            # CascadeClassifier 이용한 얼굴 탐지 (faces : 얼굴 탐지 결과 얻어진, face(x,y,w,h)로 이루어진 sequential data)\n",
    "            ## 여기에 코드 작성\n",
    "            #print(type(cap))\n",
    "            #faces = face_detector.detectMultiScale(frame)\n",
    "            faces = face_detector.detect_faces(frame)\n",
    "            \n",
    "            for face in faces:\n",
    "                \n",
    "                (x,y,w,h) = face\n",
    "            \n",
    "                # 웹캠에서 인식한 얼굴을 모델에 넣어주기 위한 전처리\n",
    "                '''\n",
    "                전처리 후, input_face에 저장\n",
    "                 1) face의 좌표에 따라 얼굴 부분 프레임만 잘라내기\n",
    "                 2) BGR2GRAY로 흑백 변환하기\n",
    "                 3) (48,48)로 resize\n",
    "                 4) 히스토그램 평활화 적용\n",
    "                 5) Tensor로 바꾸고 device에 저장\n",
    "                 6) (1,48,48)로 차원 증가\n",
    "                '''\n",
    "                ## 여기에 코드 작성\n",
    "                \n",
    "                input_face = frame[y:y+h, x:x+w]\n",
    "                #input_face = face_alignment.frontalize_face(face,frame)\n",
    "                \n",
    "                \n",
    "                input_face = cv2.cvtColor(input_face, cv2.COLOR_BGR2GRAY)\n",
    "                input_face = cv2.equalizeHist(input_face)\n",
    "                input_face = cv2.resize(input_face, (48, 48))\n",
    "                #cv2.imshow('input face', cv2.resize(input_face,(120,120)))\n",
    "                input_face = transforms.ToTensor()(input_face).to(device)\n",
    "                input_face = torch.unsqueeze(input_face, 0)\n",
    "                            \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # 모델 출력값의 shape : [1, 7, 1, 1]\n",
    "                    #emotion_vec = mini_xception(input_face).squeeze()\n",
    "                    input_face = input_face.to(device)\n",
    "                    emotion_vec = my_model(input_face).squeeze()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # 7차원 감정 확률 벡터\n",
    "                    softmax = torch.nn.Softmax()\n",
    "                    emotion_soft = softmax(emotion_vec)\n",
    "                    emotion_soft = emotion_soft.reshape(-1,1).cpu().detach().numpy()\n",
    "                    emotion_soft = np.round(emotion_soft,3)\n",
    "                    # 코사인 유사도 점수\n",
    "                    ## 여기에 코드 작성\n",
    "                    emotion_vec = emotion_vec.numpy()\n",
    "                    emotion_vec = sort_vec(emotion_vec)\n",
    "                    \n",
    "                    score = cos_sim(emotion_vec, nlp_vec)\n",
    "                    \n",
    "                    emotion_vec = torch.Tensor(emotion_vec)\n",
    "\n",
    "                    # GUI 상에서 출력할 정보\n",
    "                    '''\n",
    "                     1) 한글 문장과 최대 확률 감정, 그 확률 (이미 PIL 라이브러리로 해결)\n",
    "                     2) 코사인 유사도 점수, score\n",
    "                     3) 표정의 최대 확률 감정과 그 확률\n",
    "                    '''\n",
    "                    ## 여기에 코드 작성\n",
    "                    for i, em in enumerate(emotion_soft):\n",
    "                        em = round(em.item(),3)\n",
    "                    \n",
    "                    emotion_most = torch.argmax(emotion_vec)\n",
    "                    percentage = round(emotion_soft[emotion_most].item(), 2)\n",
    "                    emotion_most = emotion_most.squeeze().cpu().detach().item()\n",
    "                    emotion = get_label_emotion(emotion_most)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    frame[y-30:y, x:x+w] = (50,50,50)\n",
    "                    cv2.putText(frame, emotion, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,200,200))\n",
    "                    cv2.putText(frame, str(percentage), (x + w - 40,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (200,200,0))\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 3)\n",
    "                    \n",
    "                    \n",
    "                    #한글시작    \n",
    "                    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    #cv2.putText(frame_pil, \"emotion : \" + get_label_emotion(nlp_emotion_label) + \"(\" + nlp_percentage + \")\", (0, 0), font, 1, (255, 255, 255))\n",
    "                    draw = ImageDraw.Draw(frame_pil)\n",
    "                    #원래 오른쪽위\n",
    "                    #draw.text((height,0),sentence+str(score), font=font_kor,fill=(255, 0, 0))\n",
    "                    #밑으로 내리고싶어서\n",
    "                    draw.text((0,int(width/2)),sentence+str(score), font=font_kor,fill=(255, 0, 0))\n",
    "                    frame= np.array(frame_pil)\n",
    "            \n",
    "                    # 한글 문장이 출력되어 있는 프레임, frame_GUI\n",
    "                    frame_GUI = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "                    cv2.putText(frame, str(60), (10,25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0))        \n",
    "                    cv2.imshow(\"Video\", frame_GUI)\n",
    "                    #한글 끝\n",
    "    \n",
    "            \n",
    "                    \n",
    "                    \"\"\"\n",
    "                    score_box = cv2.rectangle(frame, (x,y), (x+w,y-60), (0, 0, 0), -1)\n",
    "                    \n",
    "                    \n",
    "                    # 얼굴 표정 주변의 정보 출력을 위한 OpenCV 라이브러리 사용\n",
    "                    ## 여기에 코드 작성\n",
    "                    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "                    # 탐지한 얼굴의 표정 연기 점수\n",
    "                    cv2.putText(score_box, \"Score : \" + \"[\" + str(score) + \"]\", (x, y-60), font, 1, (0, 255, 255), -1)\n",
    "                    # 탐지한 얼굴에서 최고 확률을 나타낸 감정과 그 확률\n",
    "                    cv2.putText(score_box,\"_\",  (x, y-30), font, 1, (0, 255, 255), -1) \n",
    "                    cv2.putText(score_box,\"_\",  (x+w/2, y-30), font, 1, (255, 0, 0), -1)\n",
    "\n",
    "                    #mini_xception\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "            \n",
    "            # 탈출 조건 : esc ( OxFF==27 )\n",
    "            if cv2.waitKey(1) & 0xff == 27:\n",
    "                break\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # 종료\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb826e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하고싶은 말을 입력해주세요 : 행복\n",
      "[0.11119507 0.13383299 0.1565075  0.22012912 0.14622825 0.16872445\n",
      " 0.06338271]\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
    "    if sentence == \"0\":\n",
    "        break\n",
    "    predict = predict_sentence(sentence).squeeze().cpu().detach().numpy()\n",
    "    predict_video(predict, sentence)\n",
    "    \n",
    "   # lst = np.array([0.11119507, 0.13383299, 0.1565075,  0.22012912, 0.14622825, 0.16872445,\n",
    "# 0.06338271])\n",
    "   # print(lst)\n",
    "   # predict_video(lst, sentence)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3848fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vision_main_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
